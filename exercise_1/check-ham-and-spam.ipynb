{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T18:13:55.125219Z",
     "start_time": "2025-04-15T18:13:55.121812Z"
    }
   },
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import email_read_util"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T18:13:56.884255Z",
     "start_time": "2025-04-15T18:13:56.881940Z"
    }
   },
   "source": [
    "DATA_DIR = 'trec07p/data/'\n",
    "LABELS_FILE = 'trec07p/full/index'\n",
    "TRAINING_SET_RATIO = 0.7"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T18:13:58.615669Z",
     "start_time": "2025-04-15T18:13:58.610025Z"
    }
   },
   "source": [
    "labels = {}\n",
    "spam_words = set()\n",
    "ham_words = set()"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T18:13:58.680964Z",
     "start_time": "2025-04-15T18:13:58.631077Z"
    }
   },
   "source": [
    "# Read the labels\n",
    "with open(LABELS_FILE) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        label, key = line.split()\n",
    "        labels[key.split('/')[-1]] = 1 if label.lower() == 'ham' else 0"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T18:14:00.411587Z",
     "start_time": "2025-04-15T18:14:00.371942Z"
    }
   },
   "source": [
    "# Split corpus into train and test sets\n",
    "filelist = os.listdir(DATA_DIR)\n",
    "X_train = filelist[:int(len(filelist)*TRAINING_SET_RATIO)]\n",
    "X_test = filelist[int(len(filelist)*TRAINING_SET_RATIO):]"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T18:14:02.080730Z",
     "start_time": "2025-04-15T18:14:02.076016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/artaz/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T18:29:09.028766Z",
     "start_time": "2025-04-15T18:25:11.961988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    # Parameters\n",
    "    MIN_SPAM_PERCENT = 0.01  # حداقل حضور در اسپم\n",
    "    MAX_HAM_PERCENT = 0.2   # حداکثر حضور در هم\n",
    "    LAPLACE_SMOOTHING = 1    # برای جلوگیری از تقسیم بر صفر\n",
    "\n",
    "if not os.path.exists('spam_scores1.pkl'):\n",
    "    spam_word_counts = defaultdict(int)\n",
    "    ham_word_counts = defaultdict(int)\n",
    "    total_spam = 0\n",
    "    total_ham = 0\n",
    "\n",
    "    # First pass: count words and total emails\n",
    "    for filename in X_train:\n",
    "        path = os.path.join(DATA_DIR, filename)\n",
    "        if filename in labels:\n",
    "            label = labels[filename]\n",
    "            stems = email_read_util.load(path)\n",
    "            if not stems:\n",
    "                continue\n",
    "\n",
    "            if label == 0:  # Spam\n",
    "                total_spam += 1\n",
    "                for word in set(stems):\n",
    "                    spam_word_counts[word] += 1\n",
    "            else:  # Ham\n",
    "                total_ham += 1\n",
    "                for word in set(stems):\n",
    "                    ham_word_counts[word] += 1\n",
    "\n",
    "    # Calculate spam scores for all words\n",
    "    word_scores = {}\n",
    "    for word in set(spam_word_counts.keys()).union(set(ham_word_counts.keys())):\n",
    "        spam_count = spam_word_counts.get(word, 0)\n",
    "        ham_count = ham_word_counts.get(word, 0)\n",
    "\n",
    "        # با استفاده از smoothing برای جلوگیری از تقسیم بر صفر\n",
    "        spam_prob = (spam_count + LAPLACE_SMOOTHING) / (total_spam + 2*LAPLACE_SMOOTHING)\n",
    "        ham_prob = (ham_count + LAPLACE_SMOOTHING) / (total_ham + 2*LAPLACE_SMOOTHING)\n",
    "\n",
    "        # محاسبه امتیاز اسپم به عنوان نسبت احتمال\n",
    "        word_scores[word] = spam_prob / (spam_prob + ham_prob)\n",
    "\n",
    "    pickle.dump(word_scores, open('spam_scores.pkl', 'wb'))\n",
    "else:\n",
    "    word_scores = pickle.load(open('spam_scores.pkl', 'rb'))\n",
    "\n",
    "print(f'Total spam emails in training: {total_spam}')\n",
    "print(f'Total ham emails in training: {total_ham}')\n",
    "\n",
    "# Test the model\n",
    "fp = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "THRESHOLD = 0.7  # آستانه برای تشخیص اسپم\n",
    "\n",
    "for filename in X_test:\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if filename in labels:\n",
    "        true_label = labels[filename]\n",
    "        stems = email_read_util.load(path)\n",
    "        if not stems:\n",
    "            continue\n",
    "\n",
    "        # محاسبه امتیاز کلی ایمیل\n",
    "        email_score = 0\n",
    "        found_words = 0\n",
    "        for word in set(stems):\n",
    "            if word in word_scores:\n",
    "                email_score += word_scores[word]\n",
    "                found_words += 1\n",
    "\n",
    "        if found_words > 0:\n",
    "            email_score /= found_words  # میانگین امتیاز کلمات\n",
    "\n",
    "        # پیش‌بینی بر اساس آستانه\n",
    "        predicted_label = 0 if email_score > THRESHOLD else 1\n",
    "\n",
    "        # Update confusion matrix\n",
    "        if true_label == 1 and predicted_label == 1:\n",
    "            tn += 1\n",
    "        elif true_label == 1 and predicted_label == 0:\n",
    "            fp += 1\n",
    "        elif true_label == 0 and predicted_label == 1:\n",
    "            fn += 1\n",
    "        elif true_label == 0 and predicted_label == 0:\n",
    "            tp += 1\n",
    "\n",
    "\n",
    "\n",
    "count = tn + tp + fn + fp\n",
    "# Print key metrics\n",
    "print(\"\\nKey Metrics:\")\n",
    "print(\"Classification accuracy: {}\".format(\"{:.1%}\".format((tp+tn)/count)))\n",
    "print(\"Precision (spam): {}\".format(\"{:.1%}\".format(tp/(tp+fp))))\n",
    "print(\"Recall (spam): {}\".format(\"{:.1%}\".format(tp/(tp+fn))))\n",
    "print(\"F1 Score (spam): {}\".format(\"{:.1%}\".format(2*tp/(2*tp + fp + fn))))\n",
    "print(\"False Positive Rate: {}\".format(\"{:.1%}\".format(fp/(fp+tn))))\n",
    "print(\"False Negative Rate: {}\".format(\"{:.1%}\".format(fn/(fn+tp))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total spam emails in training: 31134\n",
      "Total ham emails in training: 17615\n",
      "\n",
      "Key Metrics:\n",
      "Classification accuracy: 51.1%\n",
      "Precision (spam): 100.0%\n",
      "Recall (spam): 23.9%\n",
      "F1 Score (spam): 38.6%\n",
      "False Positive Rate: 0.0%\n",
      "False Negative Rate: 76.1%\n"
     ]
    }
   ],
   "execution_count": 71
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
