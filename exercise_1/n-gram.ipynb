{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pickle\n",
    "import email_read_util"
   ],
   "id": "883f47b6dd5c1c2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download 2007 TREC Public Spam Corpus\n",
    "1. Read the \"Agreement for use\"\n",
    "   https://plg.uwaterloo.ca/~gvcormac/treccorpus07/\n",
    "\n",
    "2. Download 255 MB Corpus (trec07p.tgz) and untar into the 'chapter1/datasets' directory\n",
    "\n",
    "3. Check that the below paths for 'DATA_DIR' and 'LABELS_FILE' exist"
   ],
   "id": "43e435d50d2f84ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DATA_DIR = 'trec07p/data/'\n",
    "LABELS_FILE = 'trec07p/full/index'\n",
    "TRAINING_SET_RATIO = 0.7"
   ],
   "id": "8506142f5274f4fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "labels = {}\n",
    "spam_words = set()\n",
    "ham_words = set()"
   ],
   "id": "d26d0e9220b006ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read the labels\n",
    "with open(LABELS_FILE) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        label, key = line.split()\n",
    "        labels[key.split('/')[-1]] = 1 if label.lower() == 'ham' else 0"
   ],
   "id": "b3c0eb5aab84f528"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split corpus into train and test sets\n",
    "filelist = os.listdir(DATA_DIR)\n",
    "X_train = filelist[:int(len(filelist)*TRAINING_SET_RATIO)]\n",
    "X_test = filelist[int(len(filelist)*TRAINING_SET_RATIO):]"
   ],
   "id": "b2f1224d0d95bf69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "b3c33bc5eb205e86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import defaultdict\n",
    "from nltk import ngrams"
   ],
   "id": "52b6a1137d0988c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5759be39a4041064"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Parameters\n",
    "NGRAM_SIZE = 2  # Use bigrams (you can change this to 3 for trigrams, etc.)\n",
    "MIN_FREQ = 5    # Minimum frequency for n-grams to be considered"
   ],
   "id": "5c4d896a68abcc92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Train the model - find frequent n-grams in spam\n",
    "if not os.path.exists('spam_ngrams.pkl'):\n",
    "    spam_ngram_counts = defaultdict(int)\n",
    "    ham_ngram_counts = defaultdict(int)\n",
    "\n",
    "    for filename in X_train:\n",
    "        path = os.path.join(DATA_DIR, filename)\n",
    "        if filename in labels:\n",
    "            label = labels[filename]\n",
    "            stems = email_read_util.load(path)\n",
    "            if not stems:\n",
    "                continue\n",
    "\n",
    "            # Generate n-grams\n",
    "            stem_ngrams = ngrams(stems, NGRAM_SIZE)\n",
    "\n",
    "            # Count n-grams based on label\n",
    "            if label == 0:  # Spam\n",
    "                for ng in stem_ngrams:\n",
    "                    spam_ngram_counts[ng] += 1\n",
    "            else:  # Ham\n",
    "                for ng in stem_ngrams:\n",
    "                    ham_ngram_counts[ng] += 1\n",
    "\n",
    "    # Filter n-grams that appear frequently in spam but rarely in ham\n",
    "    spam_indicative_ngrams = set()\n",
    "    for ng, count in spam_ngram_counts.items():\n",
    "        if count >= MIN_FREQ and ham_ngram_counts.get(ng, 0) < count/2:\n",
    "            spam_indicative_ngrams.add(ng)\n",
    "\n",
    "    pickle.dump(spam_indicative_ngrams, open('spam_ngrams.pkl', 'wb'))\n",
    "else:\n",
    "    spam_indicative_ngrams = pickle.load(open('spam_ngrams.pkl', 'rb'))\n",
    "\n",
    "\n",
    "print(f'Found {len(spam_indicative_ngrams)} spam-indicative {NGRAM_SIZE}-grams')\n"
   ],
   "id": "b10a2564eb250e61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Test the model\n",
    "fp = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "\n",
    "for filename in X_test:\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if filename in labels:\n",
    "        true_label = labels[filename]\n",
    "        stems = email_read_util.load(path)\n",
    "        if not stems:\n",
    "            continue\n",
    "\n",
    "        # Generate n-grams for this email\n",
    "        stem_ngrams = set(ngrams(stems, NGRAM_SIZE))\n",
    "\n",
    "        # Check for spam-indicative n-grams\n",
    "        spam_score = len(stem_ngrams & spam_indicative_ngrams)\n",
    "\n",
    "        # Predict spam if any spam-indicative n-grams found\n",
    "        predicted_label = 0 if spam_score > 0 else 1\n",
    "\n",
    "        # Update confusion matrix\n",
    "        if true_label == 1 and predicted_label == 1:\n",
    "            tn += 1\n",
    "        elif true_label == 1 and predicted_label == 0:\n",
    "            fp += 1\n",
    "        elif true_label == 0 and predicted_label == 1:\n",
    "            fn += 1\n",
    "        elif true_label == 0 and predicted_label == 0:\n",
    "            tp += 1\n"
   ],
   "id": "475f72c79623711a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Display results\n",
    "from IPython.display import HTML, display\n",
    "conf_matrix = [[tn, fp],\n",
    "               [fn, tp]]\n",
    "display(HTML('<table><tr>{}</tr></table>'.format(\n",
    "    '</tr><tr>'.join('<td>{}</td>'.format(\n",
    "        '</td><td>'.join(str(_) for _ in row))\n",
    "                     for row in conf_matrix))))\n",
    "\n",
    "count = tn + tp + fn + fp\n",
    "percent_matrix = [[\"{:.1%}\".format(tn/count), \"{:.1%}\".format(fp/count)],\n",
    "                  [\"{:.1%}\".format(fn/count), \"{:.1%}\".format(tp/count)]]\n",
    "display(HTML('<table><tr>{}</tr></table>'.format(\n",
    "    '</tr><tr>'.join('<td>{}</td>'.format(\n",
    "        '</td><td>'.join(str(_) for _ in row))\n",
    "                     for row in percent_matrix))))\n",
    "\n",
    "print(\"Classification accuracy: {}\".format(\"{:.1%}\".format((tp+tn)/count)))\n",
    "print(\"Precision (spam): {}\".format(\"{:.1%}\".format(tp/(tp+fp))))\n",
    "print(\"Recall (spam): {}\".format(\"{:.1%}\".format(tp/(tp+fn))))"
   ],
   "id": "39bbe67e73375ad2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
