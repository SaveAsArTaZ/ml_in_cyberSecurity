{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:09:59.617645Z",
     "start_time": "2025-04-15T09:09:59.070344Z"
    }
   },
   "cell_type": "code",
   "source": "import os",
   "id": "85b1f2a3fadf4eb2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:09:59.666642Z",
     "start_time": "2025-04-15T09:09:59.664140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = 'trec07p/data/'\n",
    "LABELS_FILE = 'trec07p/full/index'\n",
    "TRAINING_SET_RATIO = 0.7"
   ],
   "id": "be2a8b1f53a3e767",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:09:59.712140Z",
     "start_time": "2025-04-15T09:09:59.709718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = {}\n",
    "spam_words = set()\n",
    "ham_words = set()"
   ],
   "id": "6594b347de9a99a8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:09:59.827939Z",
     "start_time": "2025-04-15T09:09:59.758329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the labels\n",
    "with open(LABELS_FILE) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        label, key = line.split()\n",
    "        labels[key.split('/')[-1]] = 1 if label.lower() == 'ham' else 0"
   ],
   "id": "e6947185034395eb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:09:59.910090Z",
     "start_time": "2025-04-15T09:09:59.873123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split corpus into train and test sets\n",
    "filelist = os.listdir(DATA_DIR)\n",
    "X_train = filelist[:int(len(filelist)*TRAINING_SET_RATIO)]\n",
    "X_test = filelist[int(len(filelist)*TRAINING_SET_RATIO):]"
   ],
   "id": "267b4d4f300d2974",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:10:00.239665Z",
     "start_time": "2025-04-15T09:09:59.921062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "7d70c7cf8cf45de2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/artaz/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T18:24:54.490Z",
     "start_time": "2025-04-15T18:24:47.993030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from exercise_1.email_read_util import extract_email_text\n",
    "\n",
    "fp = 0\n",
    "tp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "\n",
    "for filename in X_test:\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if filename in labels:\n",
    "        true_label = labels[filename]\n",
    "\n",
    "        # Load the email and check if it has a subject\n",
    "        email = extract_email_text(path)\n",
    "\n",
    "        # Our prediction: 1 (ham) if subject exists, 0 (spam) if not\n",
    "        predicted_label = 0\n",
    "        if email != '':\n",
    "            predicted_label = 1\n",
    "\n",
    "        # Update confusion matrix\n",
    "        if true_label == 1 and predicted_label == 1:\n",
    "            tn += 1\n",
    "        elif true_label == 1 and predicted_label == 0:\n",
    "            fp += 1\n",
    "        elif true_label == 0 and predicted_label == 1:\n",
    "            fn += 1\n",
    "        elif true_label == 0 and predicted_label == 0:\n",
    "            tp += 1\n",
    "\n",
    "# Calculate metrics from the confusion matrix\n",
    "count = tn + fp + fn + tp\n",
    "accuracy = (tp + tn) / count\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "# Print all metrics\n",
    "print(\"\\nClassification Metrics:\")\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "print(f\"Precision (Spam): {precision:.1%}\")\n",
    "print(f\"Recall (Spam): {recall:.1%}\")\n",
    "print(f\"F1 Score (Spam): {f1:.1%}\")\n",
    "print(f\"False Positive Rate: {false_positive_rate:.1%}\")"
   ],
   "id": "2d9ad8903fc12dc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Metrics:\n",
      "True Positives (TP): 0\n",
      "True Negatives (TN): 7490\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 15136\n",
      "Accuracy: 33.1%\n",
      "Precision (Spam): 0.0%\n",
      "Recall (Spam): 0.0%\n",
      "F1 Score (Spam): 0.0%\n",
      "False Positive Rate: 0.0%\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
