{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pickle\n",
    "import email_read_util\n",
    "from matplotlib import pyplot as plt\n"
   ],
   "id": "46d8dda2172eb7b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download 2007 TREC Public Spam Corpus\n",
    "1. Read the \"Agreement for use\"\n",
    "   https://plg.uwaterloo.ca/~gvcormac/treccorpus07/\n",
    "\n",
    "2. Download 255 MB Corpus (trec07p.tgz) and untar into the 'chapter1/datasets' directory\n",
    "\n",
    "3. Check that the below paths for 'DATA_DIR' and 'LABELS_FILE' exist"
   ],
   "id": "7adb07d63cf5dafa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DATA_DIR = 'trec07p/data/'\n",
    "LABELS_FILE = 'trec07p/full/index'\n",
    "TRAINING_SET_RATIO = 0.7"
   ],
   "id": "51cc2e7412f8d208"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "labels = {}\n",
    "spam_words = set()\n",
    "ham_words = set()"
   ],
   "id": "a838b61bdab17844"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read the labels\n",
    "with open(LABELS_FILE) as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        label, key = line.split()\n",
    "        labels[key.split('/')[-1]] = 1 if label.lower() == 'ham' else 0"
   ],
   "id": "b093e87eec03b2be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split corpus into train and test sets\n",
    "filelist = os.listdir(DATA_DIR)\n",
    "X_train = filelist[:int(len(filelist)*TRAINING_SET_RATIO)]\n",
    "X_test = filelist[int(len(filelist)*TRAINING_SET_RATIO):]"
   ],
   "id": "b6abb949362192d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "bdeab3a73ea08712"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize data structures\n",
    "spam_word_counts = defaultdict(int)\n",
    "ham_word_counts = defaultdict(int)\n",
    "total_spam = 0\n",
    "total_ham = 0\n",
    "\n",
    "# Count word occurrences\n",
    "for filename in X_train:\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if filename in labels:\n",
    "        label = labels[filename]\n",
    "        stems = email_read_util.load(path)\n",
    "        if not stems:\n",
    "            continue\n",
    "\n",
    "        if label == 0:  # Spam\n",
    "            total_spam += 1\n",
    "            for word in set(stems):\n",
    "                spam_word_counts[word] += 1\n",
    "        else:  # Ham\n",
    "            total_ham += 1\n",
    "            for word in set(stems):\n",
    "                ham_word_counts[word] += 1"
   ],
   "id": "c12207db865ce85a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate word statistics\n",
    "word_stats = {}\n",
    "all_words = set(spam_word_counts.keys()).union(set(ham_word_counts.keys()))\n",
    "for word in all_words:\n",
    "    spam_count = spam_word_counts.get(word, 0)\n",
    "    ham_count = ham_word_counts.get(word, 0)\n",
    "    spam_percent = (spam_count / total_spam) * 100\n",
    "    ham_percent = (ham_count / total_ham) * 100\n",
    "    word_stats[word] = (spam_percent, ham_percent)"
   ],
   "id": "820a4e5e63d5f6d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluate different thresholds from 1% to 20%\n",
    "thresholds = range(1, 21)\n",
    "results = []\n",
    "\n",
    "for min_spam_percent in thresholds:\n",
    "    # Find words that meet the current threshold criteria\n",
    "    spam_words = set()\n",
    "    for word, (spam_p, ham_p) in word_stats.items():\n",
    "        if spam_p >= min_spam_percent:\n",
    "            spam_words.add(word)\n",
    "\n",
    "    # Test the model\n",
    "    tp = fp = fn = tn = 0\n",
    "\n",
    "    for filename in X_test:\n",
    "        path = os.path.join(DATA_DIR, filename)\n",
    "        if filename in labels:\n",
    "            true_label = labels[filename]\n",
    "            stems = email_read_util.load(path)\n",
    "            if not stems:\n",
    "                continue\n",
    "\n",
    "            # Check for spam words\n",
    "            stem_set = set(stems)\n",
    "            spam_score = len(stem_set & spam_words)\n",
    "\n",
    "            # Predict spam if any spam words found\n",
    "            predicted_label = 0 if spam_score > 0 else 1\n",
    "\n",
    "            # Update confusion matrix\n",
    "            if true_label == 1 and predicted_label == 1:\n",
    "                tn += 1\n",
    "            elif true_label == 1 and predicted_label == 0:\n",
    "                fp += 1\n",
    "            elif true_label == 0 and predicted_label == 1:\n",
    "                fn += 1\n",
    "            elif true_label == 0 and predicted_label == 0:\n",
    "                tp += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    results.append({\n",
    "        'threshold': min_spam_percent,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'spam_words_count': len(spam_words)\n",
    "    })"
   ],
   "id": "fd07c97aec0daaf7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(thresholds, [r['accuracy'] for r in results], marker='o')\n",
    "plt.title('Accuracy vs Spam Percentage Threshold')\n",
    "plt.xlabel('Minimum Spam Percentage (%)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "# Precision\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(thresholds, [r['precision'] for r in results], marker='o', color='orange')\n",
    "plt.title('Precision vs Spam Percentage Threshold')\n",
    "plt.xlabel('Minimum Spam Percentage (%)')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid(True)\n",
    "\n",
    "# Recall\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(thresholds, [r['recall'] for r in results], marker='o', color='green')\n",
    "plt.title('Recall vs Spam Percentage Threshold')\n",
    "plt.xlabel('Minimum Spam Percentage (%)')\n",
    "plt.ylabel('Recall')\n",
    "plt.grid(True)\n",
    "\n",
    "# F1 Score\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(thresholds, [r['f1'] for r in results], marker='o', color='red')\n",
    "plt.title('F1 Score vs Spam Percentage Threshold')\n",
    "plt.xlabel('Minimum Spam Percentage Threshold (%)')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(\"Threshold% | Spam Words | Accuracy | Precision | Recall | F1 Score\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "for r in results:\n",
    "    print(f\"{r['threshold']:>9}% | {r['spam_words_count']:>10} | {r['accuracy']:.3f} | {r['precision']:.3f} | {r['recall']:.3f} | {r['f1']:.3f}\")"
   ],
   "id": "ed891ba1603ef549"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
